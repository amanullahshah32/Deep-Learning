{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOayhGRO0aKCr8Fw034VLdE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanullahshah32/Deep-Learning/blob/main/PyTorch_Image_Segmentation_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YFifuLvpbU1A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super(UNET, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Down part of UNET\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Up part of UNET\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    feature * 2, feature, kernel_size=2, stride=2,\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature * 2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx // 2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx + 1](concat_skip)\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "_NmjX6aQbhDw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    x = torch.randn((3, 1, 161, 161))\n",
        "    model = UNET(in_channels=1, out_channels=1)\n",
        "    preds = model(x)\n",
        "    print(preds.shape)\n",
        "    print(x.shape)\n",
        "    assert preds.shape == x.shape\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test()\n"
      ],
      "metadata": {
        "id": "u0C2gRvY2ZAr",
        "outputId": "0cf63160-27c7-4f64-b806-d3135b5b2442",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1, 161, 161])\n",
            "torch.Size([3, 1, 161, 161])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import dataset**"
      ],
      "metadata": {
        "id": "3Qi1Dp5n2h0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "pEO3o6QK4wIa",
        "outputId": "7cf649bb-ca63-478d-a070-c21bc7b77204",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a directory for the Kaggle API credentials\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Move kaggle.json to this directory\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set the appropriate permissions\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "pAWX8LYi4xyS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c carvana-image-masking-challenge\n"
      ],
      "metadata": {
        "id": "6xfsBtwP53B1",
        "outputId": "ab8ab96c-0068-4740-fd8a-651c2a74c1fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading carvana-image-masking-challenge.zip to /content\n",
            "100% 24.4G/24.4G [04:57<00:00, 143MB/s]\n",
            "100% 24.4G/24.4G [04:57<00:00, 88.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip carvana-image-masking-challenge.zip -d carvana_data/\n"
      ],
      "metadata": {
        "id": "u9v8B1r3CboM",
        "outputId": "f0146f3b-e50d-41d7-ed71-454bc542b53e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  carvana-image-masking-challenge.zip\n",
            "  inflating: carvana_data/29bb3ece3180_11.jpg  \n",
            "  inflating: carvana_data/metadata.csv.zip  \n",
            "  inflating: carvana_data/sample_submission.csv.zip  \n",
            "  inflating: carvana_data/test.zip   \n",
            "  inflating: carvana_data/test_hq.zip  \n",
            "  inflating: carvana_data/train.zip  \n",
            "  inflating: carvana_data/train_hq.zip  \n",
            "  inflating: carvana_data/train_masks.csv.zip  \n",
            "  inflating: carvana_data/train_masks.zip  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the train and mask files\n",
        "!unzip carvana_data/train.zip -d carvana_data/train\n",
        "!unzip carvana_data/train_masks.zip -d carvana_data/train_masks\n"
      ],
      "metadata": {
        "id": "3ozda9CdExxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import dataset\n",
        "import numpy as np\n",
        "\n",
        "class CarvanaDataset(Dataset):\n",
        "  def __init__(self, image_dir, mask_dir, transform=None):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(image_dir)\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "  def __getitem__(self, index):\n",
        "    img_path = os.path.join(self.image_dir, self.images[index])\n",
        "    mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
        "    iamge = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "    mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
        "    # 0.0 , 255\n",
        "    mask[mask == 255.0] = 1.0\n",
        "    if self.transform is not None:\n",
        "      augmentations = self.transform(image=image, mask=mask)\n",
        "      image = augmentations[\"image\"]\n",
        "      mask = augmentations[\"mask\"]\n",
        "    return image, mask"
      ],
      "metadata": {
        "id": "jHRcIfpr6Euv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8"
      ],
      "metadata": {
        "id": "pSXrlYqvAFME",
        "outputId": "83baa512-f78f-4be0-a278-b9582eec6be9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "\n",
        "# Define the CarvanaDataset class\n",
        "class CarvanaDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(image_dir)  # List of image file names\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)  # Return the number of images\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
        "\n",
        "        # Load image and mask\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
        "\n",
        "        # Normalize mask to binary values (0 and 1)\n",
        "        mask[mask == 255.0] = 1.0\n",
        "\n",
        "        if self.transform is not None:\n",
        "            # Apply the transformations (augmentation) if provided\n",
        "            augmentations = self.transform(image=image, mask=mask)\n",
        "            image = augmentations[\"image\"]\n",
        "            mask = augmentations[\"mask\"]\n",
        "\n",
        "        return image, mask\n",
        "\n"
      ],
      "metadata": {
        "id": "VyaRSQm3AFc1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of applying basic PyTorch transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL Image or numpy array to tensor\n",
        "    transforms.Resize((160, 240)),  # Resize to a consistent size (optional)\n",
        "])\n",
        "\n",
        "# Create the dataset object\n",
        "train_dataset = CarvanaDataset(\n",
        "    image_dir=\"carvana_data/train\",   # Adjust the path according to your dataset\n",
        "    mask_dir=\"carvana_data/train_masks\",\n",
        "    transform=data_transforms\n",
        ")\n",
        "\n",
        "# DataLoader to load data in batches\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,  # Load images in batches of 32\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Example of checking a batch\n",
        "images, masks = next(iter(train_loader))\n",
        "print(f\"Images shape: {images.shape}\")\n",
        "print(f\"Masks shape: {masks.shape}\")\n"
      ],
      "metadata": {
        "id": "N6_sjFu9CQTE",
        "outputId": "8d041bb6-66e2-4f64-b042-297f47ef7b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'carvana_data/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-db9977e9b9e2>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create the dataset object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m train_dataset = CarvanaDataset(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mimage_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"carvana_data/train\"\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# Adjust the path according to your dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmask_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"carvana_data/train_masks\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-83802fca8fa8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_dir, mask_dir, transform)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# List of image file names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'carvana_data/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eyBswn9cENMP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}